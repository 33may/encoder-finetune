{
 "cells": [
  {
   "cell_type": "code",
   "id": "b58ed9d5313c1d06",
   "metadata": {},
   "source": [
    "import pytesseract\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Decoder\n",
    "decoder is seted up to fixed output size, so when training all images will be scaled to that size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccf24146cda349bf"
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, img_size=224, channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # The ViT uses 16x16 patches for a 224x224 image, so the patch grid is 14x14.\n",
    "        # We'll start with a 14x14 feature map and upscale step-by-step.\n",
    "        self.img_size = img_size\n",
    "        self.channels = channels\n",
    "        self.initial_resolution = 14\n",
    "        self.initial_channels = 64\n",
    "\n",
    "        # Map from embedding vector to a 14x14 feature map with 64 channels\n",
    "        self.linear = nn.Linear(embedding_dim, self.initial_channels * self.initial_resolution * self.initial_resolution)\n",
    "\n",
    "        # A series of ConvTranspose2d layers to gradually upscale 14x14 -> 28x28 -> 56x56 -> 112x112 -> 224x224\n",
    "        # kernel_size=4, stride=2, padding=1 doubles spatial dimensions\n",
    "        self.up1 = nn.ConvTranspose2d(self.initial_channels, self.initial_channels, kernel_size=4, stride=2, padding=1) # 14->28\n",
    "        self.up2 = nn.ConvTranspose2d(self.initial_channels, self.initial_channels, kernel_size=4, stride=2, padding=1) # 28->56\n",
    "        self.up3 = nn.ConvTranspose2d(self.initial_channels, self.initial_channels, kernel_size=4, stride=2, padding=1) # 56->112\n",
    "        self.up4 = nn.ConvTranspose2d(self.initial_channels, self.initial_channels, kernel_size=4, stride=2, padding=1) # 112->224\n",
    "\n",
    "        # Final convolution to reduce channels to 3 for RGB output\n",
    "        self.final_conv = nn.Conv2d(self.initial_channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Optional normalization or activation layers could be added here\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, embedding_dim]\n",
    "        # Take the CLS embedding (x[:, 0, :]) as a global image representation\n",
    "        cls_embedding = x[:, 0, :] # [batch_size, embedding_dim]\n",
    "\n",
    "        # Project to low-res feature map\n",
    "        out = self.linear(cls_embedding) # [batch_size, 64 * 14 * 14]\n",
    "        out = out.view(-1, self.initial_channels, self.initial_resolution, self.initial_resolution) # [batch, 64, 14, 14]\n",
    "\n",
    "        # Upsample steps\n",
    "        out = self.relu(self.up1(out)) # [batch, 64, 28, 28]\n",
    "        out = self.relu(self.up2(out)) # [batch, 64, 56, 56]\n",
    "        out = self.relu(self.up3(out)) # [batch, 64, 112, 112]\n",
    "        out = self.relu(self.up4(out)) # [batch, 64, 224, 224]\n",
    "\n",
    "        # Convert to 3 channels (RGB)\n",
    "        out = self.final_conv(out) # [batch, 3, 224, 224]\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4f1637aaae6777d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Foundation model\n",
    "In the model I want to freeze first layers to not lose the low level feature extraction, tuning will happen on the last layers that produce the final embedding."
   ],
   "id": "cf7c754cad82329b"
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "from torchvision.models import convnext_small, ConvNeXt_Small_Weights\n",
    "\n",
    "\n",
    "def load_latest_checkpoint(checkpoint_dir_model, checkpoint_dir_decoder, embedding_dim):\n",
    "    \n",
    "    model_checkpoints = glob.glob(f\"{checkpoint_dir_model}/checkpoint_epoch_*.pt\")\n",
    "    decoder_checkpoints = glob.glob(f\"{checkpoint_dir_decoder}/checkpoint_epoch_*.pt\")\n",
    "\n",
    "    # If no checkpoints are found, initialize a new model, decoder, and optimizer\n",
    "    if not model_checkpoints or not decoder_checkpoints:\n",
    "        print(\"No checkpoints found\")\n",
    "        model = convnext_small(weights=ConvNeXt_Small_Weights.DEFAULT)\n",
    "        decoder = SimpleDecoder(embedding_dim, img_size=420)\n",
    "        return 0, model, decoder\n",
    "\n",
    "    # Extract the epoch numbers from the checkpoint filenames\n",
    "    model_epochs = [int(f.split('_')[-1].split('.')[0]) for f in model_checkpoints]\n",
    "    decoder_epochs = [int(f.split('_')[-1].split('.')[0]) for f in decoder_checkpoints]\n",
    "\n",
    "    # Find the latest epoch that exists in both directories\n",
    "    latest_epoch = min(max(model_epochs), max(decoder_epochs))\n",
    "\n",
    "    # Construct the paths to the latest checkpoints\n",
    "    latest_model_checkpoint = f\"{checkpoint_dir_model}/checkpoint_epoch_{latest_epoch}.pt\"\n",
    "    latest_decoder_checkpoint = f\"{checkpoint_dir_decoder}/checkpoint_epoch_{latest_epoch}.pt\"\n",
    "\n",
    "    print(f\"Loading latest model checkpoint: {latest_model_checkpoint}\")\n",
    "    print(f\"Loading latest decoder checkpoint: {latest_decoder_checkpoint}\")\n",
    "\n",
    "    # Load the model and decoder\n",
    "    model = convnext_small(weights=ConvNeXt_Small_Weights.DEFAULT)\n",
    "    decoder = SimpleDecoder(embedding_dim)\n",
    "\n",
    "    # Load the checkpoints\n",
    "    model_checkpoint = torch.load(latest_model_checkpoint)\n",
    "    decoder_checkpoint = torch.load(latest_decoder_checkpoint)\n",
    "\n",
    "    # Load states into the model, decoder, and optimizer\n",
    "    model.load_state_dict(model_checkpoint['model_state_dict'])\n",
    "    decoder.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "\n",
    "    print(f\"Resumed from epoch {latest_epoch}\")\n",
    "    return latest_epoch, model, decoder\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ad9b2fb7fe22bea",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51eb8b645cd765df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "checkpoint_dir_model = \"./checkpoint_model\"\n",
    "checkpoint_dir_decoder = \"./checkpoint_decoder\"\n",
    "embedding_dim = 768\n",
    "\n",
    "start_epoch, model, decoder = load_latest_checkpoint(\n",
    "    checkpoint_dir_model,\n",
    "    checkpoint_dir_decoder,\n",
    "    embedding_dim,\n",
    ")"
   ],
   "id": "d634e945c956b0b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count how many layers (blocks) the ConvNeXt model has\n",
    "num_stages = len(model.features)  # Number of stages\n",
    "num_layers = sum(len(stage) if isinstance(stage, torch.nn.Sequential) else 1 for stage in model.features)\n",
    "\n",
    "num_layers\n"
   ],
   "id": "2cddc86a2c19179f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "freeze_layers = int(0.75 * num_layers)\n",
    "\n",
    "# Track the current layer index\n",
    "current_layer = 0\n",
    "\n",
    "# Iterate through the stages and freeze layers\n",
    "for stage in model.features:\n",
    "    if isinstance(stage, torch.nn.Sequential):\n",
    "        for layer in stage:\n",
    "            if current_layer < freeze_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "            current_layer += 1\n",
    "    else:  # If the stage is a single layer\n",
    "        if current_layer < freeze_layers:\n",
    "            for param in stage.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in stage.parameters():\n",
    "                param.requires_grad = True\n",
    "        current_layer += 1"
   ],
   "id": "c939777d472f9099",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_dim = model.classifier[-1].in_features\n",
    "embedding_dim"
   ],
   "id": "9645670306784fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def convNeXt_processor(resize_size=420):\n",
    "    return transforms.Compose([\n",
    "        # transforms.Resize(resize_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "processor = convNeXt_processor()"
   ],
   "id": "e601ece7335fb5f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "id": "7ef00b8611224f09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# folder_path = \"./nov27-train_set-labelled\"\n",
    "folder_path = \"/Users/antonnovokhatskiy/Desktop/brocvoli/nov27-train_set-labelled\"\n",
    "\n",
    "\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "image_files = [\n",
    "    f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))\n",
    "]\n",
    "\n",
    "image_arrays = []\n",
    "image_labels = []\n",
    "\n",
    "for file_name in tqdm(image_files, desc=\"Loading Images\"):\n",
    "    image_path = os.path.join(folder_path, file_name)\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert(\"RGB\")\n",
    "        img_array = np.array(img, dtype=np.uint8)\n",
    "        class_img = img_array[0:60, 0:400]\n",
    "        label = pytesseract.image_to_string(class_img, lang=\"eng\")\n",
    "        \n",
    "        img_array = img_array[60:, :, :]\n",
    "        image_arrays.append(img_array)\n",
    "        image_labels.append(label)\n",
    "\n",
    "images = image_arrays"
   ],
   "id": "ad3c72fc3112cbb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# csv_file = \"data.csv\"\n",
    "# \n",
    "# with open(csv_file, \"r\") as file:\n",
    "#     reader = csv.reader(file)\n",
    "#     shape = tuple(map(int, next(reader)))\n",
    "#     flat_images = [list(map(int, row)) for row in tqdm(reader)]\n",
    "# \n",
    "# images = [np.array(flat_image, dtype=np.uint8).reshape(shape) for flat_image in tqdm(flat_images)]"
   ],
   "id": "eb50cdf508e7cb48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class ResizingDataset(Dataset):\n",
    "    def __init__(self, images, labels, target_size=(224, 224)):\n",
    "        \"\"\"\n",
    "        Dataset class to handle resizing of images and storing labels.\n",
    "        \n",
    "        Args:\n",
    "            images (list): List of image arrays.\n",
    "            labels (list): List of labels corresponding to the images.\n",
    "            target_size (tuple): Target size for resizing the images.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PIL image for resizing\n",
    "        pil_image = Image.fromarray(img)\n",
    "        resized_image = pil_image.resize(self.target_size)\n",
    "\n",
    "        # Convert back to numpy array\n",
    "        resized_image = np.array(resized_image, dtype=np.uint8)\n",
    "\n",
    "        return resized_image, label\n",
    "\n",
    "    def get_original(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "dataset = ResizingDataset(images, image_labels, target_size=(420, 420))\n",
    "\n",
    "sampler = SequentialSampler(dataset)\n",
    "loader = DataLoader(dataset, sampler=sampler, batch_size=32)\n",
    "\n"
   ],
   "id": "329910f0625d2c87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image1, label1 = dataset[33]\n",
    "Image.fromarray(image1)"
   ],
   "id": "a6f3774c5d3ade10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image2, label2 = dataset.get_original(33)\n",
    "Image.fromarray(image2)"
   ],
   "id": "6ab2ea6a168e620b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visual setup"
   ],
   "id": "7860d6a9a40be0f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tensor_to_pil(tensor):\n",
    "    \"\"\"\n",
    "    Convert a [C, H, W] tensor in [0, 1] range to a PIL Image.\n",
    "    All operations except for the final conversion happen on GPU to leverage GPU speed,\n",
    "    then a minimal transfer to CPU is done at the end.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # If tensor is not already on GPU, move it\n",
    "        tensor = tensor.to('cuda', non_blocking=True)\n",
    "\n",
    "        # Scale and clamp while on GPU\n",
    "        tensor = tensor * 255.0\n",
    "        tensor = torch.clamp(tensor, 0, 255)\n",
    "\n",
    "        # Permute dimensions on GPU\n",
    "        tensor = tensor.permute(1, 2, 0)\n",
    "\n",
    "        # Move the final result to CPU only once\n",
    "        tensor = tensor.to('cpu', non_blocking=True).detach()\n",
    "\n",
    "        # Convert to NumPy and then to PIL image\n",
    "        np_image = tensor.numpy().astype(np.uint8)\n",
    "        pil_image = Image.fromarray(np_image)\n",
    "\n",
    "    return pil_image\n",
    "\n",
    "def show_image(original, transformed, title=\"Image\"):\n",
    "    \"\"\"\n",
    "    Display original and transformed images side-by-side.\n",
    "    Both original and transformed are assumed to be on GPU or CPU.\n",
    "    We do the minimal CPU transfer right before showing images.\n",
    "    \"\"\"\n",
    "    # Convert both images efficiently\n",
    "    # (If not needed, consider doing both conversions back-to-back to save overhead)\n",
    "    pil_image_original = tensor_to_pil(original)\n",
    "    pil_image_transformed = tensor_to_pil(transformed)\n",
    "\n",
    "    # Create a matplotlib figure\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Display the original image\n",
    "    axs[0].imshow(pil_image_original)\n",
    "    axs[0].set_title(\"Original\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Display the transformed image\n",
    "    axs[1].imshow(pil_image_transformed)\n",
    "    axs[1].set_title(\"Transformed\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "    # Clean up GPU memory after displaying\n",
    "    # If you no longer need the original or transformed tensors, delete them\n",
    "    del original, transformed\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "2980fb997ecb1383",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image, label = dataset[33]\n",
    "\n",
    "# image = image.resize((224, 224))"
   ],
   "id": "7d5ea2644e7920eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(label)\n",
    "Image.fromarray(image)"
   ],
   "id": "5ce0b8c586878b61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initial Performance"
   ],
   "id": "dedc96c6cc83d40e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "images_copy = dataset.images.copy()\n",
    "\n",
    "im_vec_initial = {\n",
    "    \"images\": images_copy,\n",
    "    \"labels\": image_labels,\n",
    "    \"embeddings\": [],\n",
    "    \"projection\": []\n",
    "}"
   ],
   "id": "595ffb5681537605",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "def compute_emebddings():\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move model to GPU once\n",
    "    model.to(device)\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in tqdm(loader, desc=\"Processing Images\"):\n",
    "\n",
    "            # inputs = torch.stack([processor(image) for image in batch_x])\n",
    "            inputs = batch_x.float() / 255.0\n",
    "            inputs = inputs.permute(0, 3, 1, 2)\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "\n",
    "            features = model.features(inputs)\n",
    "            features = model.avgpool(features)\n",
    "            embeddings = torch.flatten(features, 1) \n",
    "\n",
    "            result.extend(embeddings.detach().cpu().numpy())\n",
    "\n",
    "            del embeddings\n",
    "            del features\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return result"
   ],
   "id": "63b6b6055f47bc49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "im_vec_initial[\"embeddings\"] = compute_emebddings()",
   "id": "770994f98524a351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels to numerical values\n",
    "encoded_labels = label_encoder.fit_transform(im_vec_initial[\"labels\"])\n",
    "\n",
    "# Add the encoded labels to your dictionary\n",
    "im_vec_initial[\"labels\"] = encoded_labels\n",
    "\n",
    "# Optionally, store the mapping for future reference\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Mapping:\", label_mapping)\n"
   ],
   "id": "7f2f2d7b1d4b6eca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def k_means(embeddings, labels, projection, k=3, max_iters=100, tol=1e-4, show_plot=False):\n",
    "    # Ensure embeddings are a torch.Tensor\n",
    "    if isinstance(embeddings, list):\n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    elif isinstance(embeddings, np.ndarray):\n",
    "        embeddings = torch.from_numpy(embeddings).float()\n",
    "    elif isinstance(embeddings, torch.Tensor):\n",
    "        embeddings = embeddings.float()\n",
    "\n",
    "    # Ensure labels are a torch.Tensor\n",
    "    if isinstance(labels, list):\n",
    "        labels = np.array(labels)\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels = torch.from_numpy(labels)\n",
    "    elif isinstance(labels, torch.Tensor):\n",
    "        labels = labels\n",
    "\n",
    "    # Move embeddings and labels to GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    embeddings = embeddings.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    N, D = embeddings.shape\n",
    "\n",
    "    # Initialize centroids by selecting k random embeddings\n",
    "    torch.manual_seed(33)\n",
    "    indices = torch.randperm(N)[:k]\n",
    "    centroids = embeddings[indices].clone()\n",
    "\n",
    "    for itr in tqdm(range(max_iters), desc=\"K-Means Clustering\"):\n",
    "        # Compute distances between embeddings and centroids using (a - b)^2 = a^2 + b^2 - 2ab\n",
    "        embeddings_squared = torch.sum(embeddings ** 2, dim=1, keepdim=True)  # [N, 1]\n",
    "        centroids_squared = torch.sum(centroids ** 2, dim=1)  # [k]\n",
    "        distances = embeddings_squared + centroids_squared - 2 * torch.matmul(embeddings, centroids.t())  # [N, k]\n",
    "\n",
    "        # Assign clusters based on closest centroid\n",
    "        cluster_assignments = torch.argmin(distances, dim=1)  # [N]\n",
    "\n",
    "        # Compute new centroids as the mean of assigned embeddings\n",
    "        new_centroids = torch.zeros_like(centroids)\n",
    "        for cluster in range(k):\n",
    "            assigned_embeddings = embeddings[cluster_assignments == cluster]\n",
    "            if assigned_embeddings.shape[0] > 0:\n",
    "                new_centroids[cluster] = assigned_embeddings.mean(dim=0)\n",
    "            else:\n",
    "                # If a cluster lost all its members, reinitialize it randomly\n",
    "                new_centroids[cluster] = embeddings[torch.randint(0, N, (1,))]\n",
    "\n",
    "        # Check for convergence\n",
    "        centroid_shift = torch.norm(new_centroids - centroids, dim=1).mean().item()\n",
    "        centroids = new_centroids.clone()\n",
    "        if centroid_shift < tol:\n",
    "            print(f\"Converged at iteration {itr}\")\n",
    "            break\n",
    "\n",
    "        # Clean up to save memory\n",
    "        del distances, cluster_assignments, assigned_embeddings, new_centroids\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Move data back to CPU for further processing\n",
    "    centroids = centroids.to('cpu')\n",
    "    cluster_assignments = cluster_assignments.to('cpu')\n",
    "    labels = labels.to('cpu')\n",
    "\n",
    "    # Assign labels to clusters based on majority vote\n",
    "    label_mapping = {}\n",
    "    for cluster in range(k):\n",
    "        assigned_labels = labels[cluster_assignments == cluster]\n",
    "        if len(assigned_labels) == 0:\n",
    "            label_mapping[cluster] = -1  # Unassigned\n",
    "            continue\n",
    "        most_common = torch.bincount(assigned_labels).argmax().item()\n",
    "        label_mapping[cluster] = most_common\n",
    "\n",
    "    # Assign cluster labels based on mapping\n",
    "    assigned_cluster_labels = [label_mapping[cluster.item()] for cluster in cluster_assignments]\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = np.mean(np.array(assigned_cluster_labels) == labels.numpy())\n",
    "\n",
    "    # Optionally, plot the clusters using UMAP projections\n",
    "    if show_plot:\n",
    "        if projection is None:\n",
    "            raise ValueError(\"UMAP projections not provided. Please compute and pass the projection.\")\n",
    "\n",
    "        if isinstance(projection, torch.Tensor):\n",
    "            projection = projection.cpu().numpy()\n",
    "        elif isinstance(projection, list):\n",
    "            projection = np.array(projection)\n",
    "        elif not isinstance(projection, np.ndarray):\n",
    "            raise ValueError(\"Projection must be a numpy array or torch.Tensor\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot K-Means Clusters\n",
    "        plt.subplot(1, 2, 1)\n",
    "        scatter = plt.scatter(projection[:, 0], projection[:, 1], c=assigned_cluster_labels, cmap='viridis', alpha=0.6)\n",
    "        plt.title(\"K-Means Clustering Results (UMAP Projection)\")\n",
    "        plt.xlabel(\"UMAP Component 1\")\n",
    "        plt.ylabel(\"UMAP Component 2\")\n",
    "        plt.colorbar(scatter, label='Cluster Label')\n",
    "\n",
    "        # Plot Original Classes\n",
    "        plt.subplot(1, 2, 2)\n",
    "        scatter = plt.scatter(projection[:, 0], projection[:, 1], c=labels.numpy(), cmap='viridis', alpha=0.6)\n",
    "        plt.title(\"Original Class Labels (UMAP Projection)\")\n",
    "        plt.xlabel(\"UMAP Component 1\")\n",
    "        plt.ylabel(\"UMAP Component 2\")\n",
    "        plt.colorbar(scatter, label='Original Label')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Clean up GPU memory\n",
    "    del embeddings, cluster_assignments, labels\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\n",
    "        \"cluster_labels\": assigned_cluster_labels,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"centroids\": centroids,\n",
    "        \"label_mapping\": label_mapping\n",
    "    }\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f44af9bb75ee398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from umap import UMAP\n",
    "import numpy as np\n",
    "\n",
    "def compute_umap_embeddings(embeddings, dimensions=2):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        embeddings (array-like): The original high-dimensional embeddings.\n",
    "        dimensions (int): The number of dimensions for UMAP reduction (default is 2).\n",
    "    \"\"\"\n",
    "    reducer = UMAP(n_components=dimensions, random_state=42)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    return reduced_embeddings\n"
   ],
   "id": "7325e061c62184f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embeddings = np.array(im_vec_initial[\"embeddings\"])\n",
    "images = im_vec_initial[\"images\"]\n",
    "\n",
    "reduced_embeddings = compute_umap_embeddings(embeddings, dimensions=2)\n",
    "\n",
    "im_vec_initial[\"projection\"] =reduced_embeddings"
   ],
   "id": "ab55cbcb5d146e65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = k_means(\n",
    "    embeddings=im_vec_initial[\"embeddings\"],\n",
    "    labels=im_vec_initial[\"labels\"],\n",
    "    projection=im_vec_initial[\"projection\"],\n",
    "    k=3,           \n",
    "    max_iters=100,    \n",
    "    tol=1e-2,         \n",
    "    show_plot=True   \n",
    ")\n",
    "\n",
    "print(f\"Clustering Accuracy: {results['accuracy'] * 100:.2f}%\")"
   ],
   "id": "534f8cd624192df0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "def plot_umap_with_images(reduced_embeddings, images):\n",
    "    \"\"\"\n",
    "    Plots UMAP-reduced embeddings with interactive selection to show corresponding images in a grid.\n",
    "    \n",
    "    Args:\n",
    "        reduced_embeddings (np.ndarray): The reduced embeddings from UMAP with shape (n_samples, 2).\n",
    "        images (list): List of images (as NumPy arrays) corresponding to the embeddings.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not isinstance(reduced_embeddings, np.ndarray):\n",
    "        raise ValueError(\"reduced_embeddings must be a NumPy array.\")\n",
    "    if reduced_embeddings.shape[1] != 2:\n",
    "        raise ValueError(\"reduced_embeddings must have shape (n_samples, 2).\")\n",
    "    if len(reduced_embeddings) != len(images):\n",
    "        raise ValueError(\"The number of embeddings must match the number of images.\")\n",
    "\n",
    "    # Initialize a set to keep track of selected indices\n",
    "    selected_indices = set()\n",
    "\n",
    "    # Create a FigureWidget for interactivity\n",
    "    fig = go.FigureWidget(\n",
    "        data=go.Scatter(\n",
    "            x=reduced_embeddings[:, 0],\n",
    "            y=reduced_embeddings[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                color=['blue'] * len(images),  # Initial color for all points\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            customdata=list(range(len(images))),  # Store image indices\n",
    "            hoverinfo='text',\n",
    "            hovertext=[f'Image Index: {i}' for i in range(len(images))]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout to make the plot square\n",
    "    fig.update_layout(\n",
    "        title=\"UMAP Visualization\",\n",
    "        width=800,\n",
    "        height=800,\n",
    "        xaxis_title=\"UMAP 1\",\n",
    "        yaxis_title=\"UMAP 2\",\n",
    "        showlegend=False,\n",
    "        margin=dict(l=40, r=40, t=40, b=40),\n",
    "        dragmode='lasso'  # Set default drag mode to lasso for multiple selections\n",
    "    )\n",
    "\n",
    "    # Create an Output widget to display images as a grid\n",
    "    image_output = widgets.Output(layout={\n",
    "        'border': '1px solid black',\n",
    "        'width': '1500px',\n",
    "        'height': '800px',\n",
    "        'overflow': 'auto'\n",
    "    })\n",
    "\n",
    "    # Function to update image grid based on selected indices\n",
    "    def update_image_grid(indices):\n",
    "        with image_output:\n",
    "            image_output.clear_output()\n",
    "            if not indices:\n",
    "                display(widgets.HTML(\"<b>No images selected.</b>\"))\n",
    "                return\n",
    "            n_cols = 6\n",
    "            img_widgets = []\n",
    "            for idx in sorted(indices):\n",
    "                img_array = images[idx]\n",
    "                # Ensure the image is in uint8 format\n",
    "                if img_array.dtype != np.uint8:\n",
    "                    img_array = img_array.astype(np.uint8)\n",
    "                img = Image.fromarray(img_array)\n",
    "                # Optionally resize image for better display\n",
    "                buffer = io.BytesIO()\n",
    "                img.save(buffer, format='PNG')\n",
    "                img_bytes = buffer.getvalue()\n",
    "                # Create Image widget\n",
    "                img_widget = widgets.Image(\n",
    "                    value=img_bytes,\n",
    "                    format='png',\n",
    "                    width=398,\n",
    "                    height=224\n",
    "                )\n",
    "                img_widgets.append(img_widget)\n",
    "            # Create GridBox layout\n",
    "            grid = widgets.GridBox(\n",
    "                img_widgets,\n",
    "                layout=widgets.Layout(\n",
    "                    grid_template_columns=f\"repeat({n_cols}, 224px)\",\n",
    "                    grid_gap='10px'\n",
    "                )\n",
    "            )\n",
    "            display(grid)\n",
    "\n",
    "    # Function to handle click events (toggle selection)\n",
    "    def on_click(trace, points, state):\n",
    "        if points.point_inds:\n",
    "            for idx in points.point_inds:\n",
    "                if idx in selected_indices:\n",
    "                    selected_indices.remove(idx)\n",
    "                else:\n",
    "                    selected_indices.add(idx)\n",
    "            # Update marker colors based on selection\n",
    "            with fig.batch_update():\n",
    "                trace.marker.color = [\n",
    "                    'red' if i in selected_indices else 'blue' for i in range(len(images))\n",
    "                ]\n",
    "            update_image_grid(selected_indices)\n",
    "\n",
    "    # Function to handle selection events (lasso or box select)\n",
    "    def on_select(trace, points, state):\n",
    "        if points.point_inds:\n",
    "            # Replace current selection with new selection\n",
    "            selected_indices.clear()\n",
    "            for idx in points.point_inds:\n",
    "                selected_indices.add(idx)\n",
    "            # Update marker colors based on selection\n",
    "            with fig.batch_update():\n",
    "                trace.marker.color = [\n",
    "                    'red' if i in selected_indices else 'blue' for i in range(len(images))\n",
    "                ]\n",
    "            update_image_grid(selected_indices)\n",
    "        else:\n",
    "            # If no points selected, clear selection\n",
    "            selected_indices.clear()\n",
    "            with fig.batch_update():\n",
    "                trace.marker.color = ['blue'] * len(images)\n",
    "            update_image_grid(selected_indices)\n",
    "\n",
    "    # Attach the click and select events to the scatter plot\n",
    "    fig.data[0].on_click(on_click)\n",
    "    fig.data[0].on_selection(on_select)\n",
    "\n",
    "    # Initial display message\n",
    "    with image_output:\n",
    "        display(widgets.HTML(\"<b>No images selected.</b>\"))\n",
    "\n",
    "    # Layout the plot and image grid side by side\n",
    "    hbox = widgets.HBox([fig, image_output])\n",
    "    display(hbox)\n"
   ],
   "id": "757d213ecd72b4cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_umap_with_images(reduced_embeddings, images)"
   ],
   "id": "b21b2dad2c34e94c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def similarity(query_index, data_dict, k=10):\n",
    "    \"\"\"    \n",
    "    Args:\n",
    "        query_index (int): Index of the query image in the data_dict.\n",
    "        data_dict (dict): \n",
    "            \"images\": List of images as NumPy arrays with shape (224, 398, 3).\n",
    "            \"embeddings\": List of embedding vectors, each of shape (768,).\n",
    "        k (int): Number of top similar images to return. Default is 10.\n",
    "    \"\"\"\n",
    "    embeddings_matrix = np.array(data_dict[\"embeddings\"])  # Shape: (n_samples, 768)\n",
    "\n",
    "    query_embedding = embeddings_matrix[query_index]        # Shape: (768,)\n",
    "    query_image = data_dict[\"images\"][query_index]           # Shape: (224, 398, 3)\n",
    "\n",
    "    embeddings_norm = embeddings_matrix / np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)\n",
    "    query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "    cosine_similarities = np.dot(embeddings_norm, query_norm)  # Shape: (n_samples,)\n",
    "\n",
    "    # Exclude the query itself\n",
    "    cosine_similarities[query_index] = -np.inf\n",
    "\n",
    "    # Get top k indices\n",
    "    top_k_indices = np.argsort(cosine_similarities)[-k:][::-1]  # Descending order\n",
    "\n",
    "    # Retrieve top k images and their similarity scores\n",
    "    top_k_images = [data_dict[\"images\"][i] for i in top_k_indices]\n",
    "    top_k_scores = [cosine_similarities[i] for i in top_k_indices]\n",
    "\n",
    "    plt.figure(figsize=(5 * (k + 1), 5)) \n",
    "\n",
    "    # Display the original (query) image\n",
    "    plt.subplot(1, k + 1, 1)\n",
    "    plt.imshow(query_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\", fontsize=20)\n",
    "\n",
    "    # Display the top k similar images\n",
    "    for idx, (img, score) in enumerate(zip(top_k_images, top_k_scores), start=2):\n",
    "        plt.subplot(1, k + 1, idx)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{score:.4f}\", fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Combine images and scores\n",
    "    top_k_results = list(zip(top_k_images, top_k_scores))\n",
    "\n",
    "    return top_k_results\n",
    "\n",
    "\n",
    "def similarity_range(data_dict):\n",
    "    query_embedding = np.ones(768, dtype=np.float32)\n",
    "    \n",
    "    embeddings_matrix = np.array(data_dict[\"embeddings\"])  # Shape: (n_samples, 768)\n",
    "\n",
    "    embeddings_norm = embeddings_matrix / np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)\n",
    "    query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "    cosine_similarities = np.dot(embeddings_norm, query_norm)  # Shape: (n_samples,)\n",
    "\n",
    "    return np.min(cosine_similarities), np.max(cosine_similarities)\n",
    "\n",
    "similarity_range(im_vec_initial)"
   ],
   "id": "62cacde1307db8d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "similarity_range(im_vec_initial)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe8af632a96da734",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Similarity Range\n",
    "So this is the higlight of the problem. Since the foundational model is trained and designed to be used on diverse datasets, when inserted one datasets, items from it falls in tiny regoion of space. This is what is tended to be fixed, we want to increse the similarity scores range of the data, thus give more space for features to represent features that are relevant to this specific problem."
   ],
   "id": "96a6edec22d98cac"
  },
  {
   "cell_type": "code",
   "source": [
    "top_k_results = similarity(650, im_vec_initial, k=10)\n",
    "top_k_results = similarity(66, im_vec_initial, k=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90ada78eb02dfb3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "The idea is to fine-tune the foundation model in unsupervised way with idea of encoder and decoder. Intuition is as follows - in this task the main idea is try to overfit the data as much as possible, since we dont use this embeddings in any multi-dataset tasks or meming retrieval, we only want embeddings to be good comparable to each other. This implies that if the decoder model could overfit on the main idea of dataset and image view, then embedding from encoder model will describe all the features that differ one image from another."
   ],
   "id": "d7b2031a42e983d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setup Train\n",
    "choose parameters to train on both Encoder and Decoder"
   ],
   "id": "31e83ee06fe98b3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "params_to_optimize = list(decoder.parameters())\n",
    "\n",
    "# Add the trainable parameters from the last 25% of ViT layers\n",
    "for i, layer in enumerate(model.encoder.layer):\n",
    "    if i >= freeze_layers:\n",
    "        params_to_optimize.extend(list(layer.parameters()))"
   ],
   "id": "17dd6c87346460e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(params_to_optimize, lr=1e-3)\n",
    "criterion = nn.MSELoss()"
   ],
   "id": "f469e667553f63a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19724f177740ef33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm  # Optional: For progress bars\n",
    "\n",
    "\n",
    "def train(num_epochs):\n",
    "    \"\"\"\n",
    "    Trains the model and decoder for a specified number of epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - num_epochs (int): The number of epochs to train the model.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(checkpoint_dir_model, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir_decoder, exist_ok=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Setup Device (CUDA if available)\n",
    "    # -------------------------------\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Move the model and decoder to the selected device\n",
    "    model.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Training Loop Over Epochs\n",
    "    # -------------------------------\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "        epoch_loss = 0.0  # To accumulate loss over the epoch\n",
    "\n",
    "        # Set models to training mode\n",
    "        model.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Optional: Use tqdm for a progress bar\n",
    "        progress_bar = tqdm(enumerate(loader), total=len(loader), desc=f\"Training Epoch {epoch}\")\n",
    "\n",
    "        for batch_idx, data in progress_bar:\n",
    "            # -------------------------------\n",
    "            # 3. Prepare the Batch\n",
    "            # -------------------------------\n",
    "            # Assume batch shape is [Batch, Height, Width, Channels]\n",
    "            # Permute to [Batch, Channels, Height, Width]\n",
    "            \n",
    "            batch_x, batch_y = data\n",
    "\n",
    "            batch_x = batch_x.permute(0, 3, 1, 2)  # New shape: [Batch, 3, 224, 224]\n",
    "\n",
    "            # Convert to float and normalize to [0, 1]\n",
    "            batch_x = batch_x.float() / 255.0\n",
    "\n",
    "            # Define target as the input batch (for reconstruction)\n",
    "            target = batch_x.clone()\n",
    "\n",
    "            # Move batch and target to the device\n",
    "            batch_x = batch_x.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # -------------------------------\n",
    "            # 4. Forward Pass\n",
    "            # -------------------------------\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            outputs = model(batch_x, output_hidden_states=False)  # Forward pass through the model\n",
    "            reconstruction = decoder(outputs.last_hidden_state)  # Decode the model's output\n",
    "\n",
    "            # -------------------------------\n",
    "            # 5. Compute Loss\n",
    "            # -------------------------------\n",
    "            loss = criterion(reconstruction, target)\n",
    "            epoch_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "            # -------------------------------\n",
    "            # 6. Backward Pass and Optimization\n",
    "            # -------------------------------\n",
    "            loss.backward()  # Backward pass to compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            # -------------------------------\n",
    "            # 7. Logging and Visualization\n",
    "            # -------------------------------\n",
    "            # Update the progress bar with the current loss\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "        # -------------------------------\n",
    "        # 8. Epoch Summary\n",
    "        # -------------------------------\n",
    "        avg_loss_start = time.time()\n",
    "        avg_epoch_loss = epoch_loss / len(loader)\n",
    "        avg_loss_end = time.time()\n",
    "    \n",
    "        # Epoch completion timing\n",
    "        end_epoch_time = time.time()\n",
    "        print(f\"(Avg Loss Calculation: {avg_loss_end - avg_loss_start:.4f}s). Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if epoch % 1 == 0:\n",
    "            # Model checkpoint\n",
    "            start_model_checkpoint = time.time()\n",
    "            # checkpoint_path_model = os.path.join(checkpoint_dir_model, f'checkpoint_epoch_{epoch}.pt')\n",
    "            # torch.save({\n",
    "            #     'epoch': epoch,\n",
    "            #     'model_state_dict': model.state_dict(),\n",
    "            #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #     'loss': avg_epoch_loss,\n",
    "            # }, checkpoint_path_model)\n",
    "            # end_model_checkpoint = time.time()\n",
    "            # print(f'Model Checkpoint Saved: {checkpoint_path_model} (Time: {end_model_checkpoint - start_model_checkpoint:.4f}s)')\n",
    "    \n",
    "            # Decoder checkpoint\n",
    "            # start_decoder_checkpoint = time.time()\n",
    "            # checkpoint_path_decoder = os.path.join(checkpoint_dir_decoder, f'checkpoint_epoch_{epoch}.pt')\n",
    "            # torch.save({\n",
    "            #     'epoch': epoch,\n",
    "            #     'model_state_dict': model.state_dict(),  # Adjust if separate decoder exists\n",
    "            #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #     'loss': avg_epoch_loss,\n",
    "            # }, checkpoint_path_decoder)\n",
    "            # end_decoder_checkpoint = time.time()\n",
    "            # print(f'Decoder Checkpoint Saved: {checkpoint_path_decoder} (Time: {end_decoder_checkpoint - start_decoder_checkpoint:.4f}s)')\n",
    "\n",
    "\n",
    "            emebddings = compute_emebddings()\n",
    "            projection = compute_umap_embeddings(embeddings)\n",
    "            \n",
    "            results = k_means(\n",
    "                embeddings=emebddings,\n",
    "                labels=im_vec_initial[\"encoded_labels\"],\n",
    "                projection=projection,\n",
    "                k=3,\n",
    "                max_iters=100,\n",
    "                tol=1e-2,\n",
    "                show_plot=True\n",
    "            )\n",
    "            print(f\"Clustering Accuracy: {results['accuracy'] * 100:.2f}%\")\n",
    "            \n",
    "            show_image(batch_x[0] ,reconstruction[0])\n",
    "            \n",
    "    \n",
    "        print('-' * 50)  # Separator for readability\n"
   ],
   "id": "7490ba27563cea67",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train(40)",
   "metadata": {
    "collapsed": false
   },
   "id": "3dc1564a51d0994c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reconstructed_images = decoder(outputs.last_hidden_state)"
   ],
   "id": "7f6ec7318726f09a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [],
   "id": "5705fad82411ec10",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
