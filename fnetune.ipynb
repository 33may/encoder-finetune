{
 "cells": [
  {
   "cell_type": "code",
   "id": "b58ed9d5313c1d06",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Foundation model\n",
    "In the model I want to freeze first layers to not lose the low level feature extraction, tuning will happen on the last layers that produce the final embedding."
   ],
   "id": "cf7c754cad82329b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the pretrained ViT model\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ],
   "id": "d634e945c956b0b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count how many layers the encoder has\n",
    "num_layers = len(model.encoder.layer)  # Typically 12 for vit-base"
   ],
   "id": "2cddc86a2c19179f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Freeze the first 75% layers\n",
    "freeze_layers = int(0.75 * num_layers)  # e.g., 9 if num_layers=12\n",
    "for i, layer in enumerate(model.encoder.layer):\n",
    "    if i < freeze_layers:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        # The last 25% remain trainable\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True"
   ],
   "id": "c939777d472f9099",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "embedding_dim = model.config.hidden_size",
   "id": "9645670306784fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')",
   "id": "e601ece7335fb5f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup Decoder\n",
    "decoder is seted up to fixed output size, so when training all images will be scaled to that size"
   ],
   "id": "af48d5762886c2d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, img_size=224, channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # The ViT uses 16x16 patches for a 224x224 image, so the patch grid is 14x14.\n",
    "        # We'll start with a 14x14 feature map and upscale step-by-step.\n",
    "        self.img_size = img_size\n",
    "        self.channels = channels\n",
    "        self.initial_resolution = 14\n",
    "        self.initial_channels = 64\n",
    "\n",
    "        # Map from embedding vector to a 14x14 feature map with 64 channels\n",
    "        self.linear = nn.Linear(embedding_dim, self.initial_channels * self.initial_resolution * self.initial_resolution)\n",
    "\n",
    "        # A series of ConvTranspose2d layers to gradually upscale 14x14 -> 28x28 -> 56x56 -> 112x112 -> 224x224\n",
    "        # kernel_size=4, stride=2, padding=1 doubles spatial dimensions\n",
    "        self.up1 = nn.ConvTranspose2d(self.initial_channels, self.initial_channels, kernel_size=4, stride=2, padding=1) # 14->28\n",
    "        self.up2 = nn.ConvTranspose2d(self.initial_channels, self.initial_channels, kernel_size=4, stride=2, padding=1) # 28->56\n",
    "        self.up3 = nn.ConvTranspose2d(self.initial_channels, self.initial_channels, kernel_size=4, stride=2, padding=1) # 56->112\n",
    "        self.up4 = nn.ConvTranspose2d(self.initial_channels, self.initial_channels, kernel_size=4, stride=2, padding=1) # 112->224\n",
    "\n",
    "        # Final convolution to reduce channels to 3 for RGB output\n",
    "        self.final_conv = nn.Conv2d(self.initial_channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Optional normalization or activation layers could be added here\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, embedding_dim]\n",
    "        # Take the CLS embedding (x[:, 0, :]) as a global image representation\n",
    "        cls_embedding = x[:, 0, :] # [batch_size, embedding_dim]\n",
    "\n",
    "        # Project to low-res feature map\n",
    "        out = self.linear(cls_embedding) # [batch_size, 64 * 14 * 14]\n",
    "        out = out.view(-1, self.initial_channels, self.initial_resolution, self.initial_resolution) # [batch, 64, 14, 14]\n",
    "\n",
    "        # Upsample steps\n",
    "        out = self.relu(self.up1(out)) # [batch, 64, 28, 28]\n",
    "        out = self.relu(self.up2(out)) # [batch, 64, 56, 56]\n",
    "        out = self.relu(self.up3(out)) # [batch, 64, 112, 112]\n",
    "        out = self.relu(self.up4(out)) # [batch, 64, 224, 224]\n",
    "\n",
    "        # Convert to 3 channels (RGB)\n",
    "        out = self.final_conv(out) # [batch, 3, 224, 224]\n",
    "\n",
    "        return out"
   ],
   "id": "6667d87b9272c2ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "decoder = SimpleDecoder(embedding_dim)",
   "id": "bea7601c67cc07a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Dataset",
   "id": "7ef00b8611224f09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "csv_file = \"data.csv\"\n",
    "\n",
    "with open(csv_file, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    shape = tuple(map(int, next(reader)))\n",
    "    flat_images = [list(map(int, row)) for row in tqdm(reader)]\n",
    "\n",
    "images = [np.array(flat_image, dtype=np.uint8).reshape(shape) for flat_image in tqdm(flat_images)]"
   ],
   "id": "eb50cdf508e7cb48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "\n",
    "class ResizingDataset(Dataset):\n",
    "    def __init__(self, images, target_size=(224, 224)):\n",
    "        self.images = images\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        pil_image = Image.fromarray(img)\n",
    "        resized_image = pil_image.resize(self.target_size)\n",
    "\n",
    "        return np.array(resized_image, dtype=np.uint8)\n",
    "\n",
    "    def get_original(self, idx):\n",
    "        return self.images[idx]\n",
    "\n",
    "dataset = ResizingDataset(images, target_size=(224, 224))\n",
    "sampler = SequentialSampler(dataset)\n",
    "loader = DataLoader(dataset, sampler=sampler, batch_size=4)"
   ],
   "id": "329910f0625d2c87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image1 = Image.fromarray(dataset[33])\n",
    "image1"
   ],
   "id": "a6f3774c5d3ade10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image2 = Image.fromarray(dataset.get_original(33))\n",
    "image2"
   ],
   "id": "6ab2ea6a168e620b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visual setup",
   "id": "7860d6a9a40be0f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tensor_to_pil(tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Tensor of shape [C, H, W] with values in [0, 1].\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Converted image.\n",
    "    \"\"\"\n",
    "    # Move tensor to CPU and detach from the computation graph\n",
    "    tensor = tensor.cpu().detach()\n",
    "    # Clamp the tensor to ensure all values are within [0, 1]\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    \n",
    "    np_image = tensor.numpy()\n",
    "    # Transpose the array from [C, H, W] to [H, W, C] for PIL\n",
    "    np_image = np.transpose(np_image, (1, 2, 0))\n",
    "    \n",
    "    np_image = (np_image * 255).astype(np.uint8)\n",
    "    \n",
    "    pil_image = Image.fromarray(np_image)\n",
    "\n",
    "    return pil_image\n",
    "\n",
    "\n",
    "def show_image(orginal, transformed, title=\"Image\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Image tensor of shape [3, H, W]\n",
    "        title (str): Title for the displayed image\n",
    "    \"\"\"\n",
    "    # Convert tensor to PIL Image\n",
    "    pil_image_original = tensor_to_pil(orginal)\n",
    "    pil_image_otransformed = tensor_to_pil(transformed)\n",
    "\n",
    "    # Create a matplotlib figure\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Display the image\n",
    "    axs[0].imshow(pil_image_original)\n",
    "    axs[0].set_title(\"original\")\n",
    "    axs[0].axis('off')  # Hide axis\n",
    "\n",
    "    # Display the transformed image\n",
    "    axs[1].imshow(pil_image_otransformed)\n",
    "    axs[1].set_title(\"transformed\")\n",
    "    axs[1].axis('off')  # Hide axis\n",
    "\n",
    "    # Show the image inline\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "2980fb997ecb1383",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image = Image.fromarray(dataset[33])\n",
    "\n",
    "# image = image.resize((224, 224))"
   ],
   "id": "7d5ea2644e7920eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "image",
   "id": "5ce0b8c586878b61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initial Performance",
   "id": "dedc96c6cc83d40e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "images_copy = dataset.images.copy()\n",
    "\n",
    "im_vec_initial = {\n",
    "    \"images\": images_copy,\n",
    "    \"embeddings\": []\n",
    "}"
   ],
   "id": "595ffb5681537605",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "for batch in tqdm(loader, desc=\"Processing Images\"):\n",
    "    inputs = processor(images=batch, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_hidden_states=False)\n",
    "    \n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    im_vec_initial[\"embeddings\"].extend(embeddings.detach().cpu().numpy())\n",
    "    "
   ],
   "id": "770994f98524a351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from umap import UMAP\n",
    "import numpy as np\n",
    "\n",
    "def compute_umap_embeddings(embeddings, dimensions=2):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        embeddings (array-like): The original high-dimensional embeddings.\n",
    "        dimensions (int): The number of dimensions for UMAP reduction (default is 2).\n",
    "    \"\"\"\n",
    "    reducer = UMAP(n_components=dimensions, random_state=42)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    return reduced_embeddings\n"
   ],
   "id": "7325e061c62184f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embeddings = np.array(im_vec_initial[\"embeddings\"])\n",
    "images = im_vec_initial[\"images\"]\n",
    "\n",
    "reduced_embeddings = compute_umap_embeddings(embeddings, dimensions=2)"
   ],
   "id": "ab55cbcb5d146e65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "def plot_umap_with_images(reduced_embeddings, images):\n",
    "    \"\"\"\n",
    "    Plots UMAP-reduced embeddings with interactive selection to show corresponding images in a grid.\n",
    "    \n",
    "    Args:\n",
    "        reduced_embeddings (np.ndarray): The reduced embeddings from UMAP with shape (n_samples, 2).\n",
    "        images (list): List of images (as NumPy arrays) corresponding to the embeddings.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not isinstance(reduced_embeddings, np.ndarray):\n",
    "        raise ValueError(\"reduced_embeddings must be a NumPy array.\")\n",
    "    if reduced_embeddings.shape[1] != 2:\n",
    "        raise ValueError(\"reduced_embeddings must have shape (n_samples, 2).\")\n",
    "    if len(reduced_embeddings) != len(images):\n",
    "        raise ValueError(\"The number of embeddings must match the number of images.\")\n",
    "\n",
    "    # Initialize a set to keep track of selected indices\n",
    "    selected_indices = set()\n",
    "\n",
    "    # Create a FigureWidget for interactivity\n",
    "    fig = go.FigureWidget(\n",
    "        data=go.Scatter(\n",
    "            x=reduced_embeddings[:, 0],\n",
    "            y=reduced_embeddings[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                color=['blue'] * len(images),  # Initial color for all points\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            customdata=list(range(len(images))),  # Store image indices\n",
    "            hoverinfo='text',\n",
    "            hovertext=[f'Image Index: {i}' for i in range(len(images))]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout to make the plot square\n",
    "    fig.update_layout(\n",
    "        title=\"UMAP Visualization\",\n",
    "        width=800,\n",
    "        height=800,\n",
    "        xaxis_title=\"UMAP 1\",\n",
    "        yaxis_title=\"UMAP 2\",\n",
    "        showlegend=False,\n",
    "        margin=dict(l=40, r=40, t=40, b=40),\n",
    "        dragmode='lasso'  # Set default drag mode to lasso for multiple selections\n",
    "    )\n",
    "\n",
    "    # Create an Output widget to display images as a grid\n",
    "    image_output = widgets.Output(layout={\n",
    "        'border': '1px solid black',\n",
    "        'width': '1500px',\n",
    "        'height': '800px',\n",
    "        'overflow': 'auto'\n",
    "    })\n",
    "\n",
    "    # Function to update image grid based on selected indices\n",
    "    def update_image_grid(indices):\n",
    "        with image_output:\n",
    "            image_output.clear_output()\n",
    "            if not indices:\n",
    "                display(widgets.HTML(\"<b>No images selected.</b>\"))\n",
    "                return\n",
    "            n_cols = 6\n",
    "            img_widgets = []\n",
    "            for idx in sorted(indices):\n",
    "                img_array = images[idx]\n",
    "                # Ensure the image is in uint8 format\n",
    "                if img_array.dtype != np.uint8:\n",
    "                    img_array = img_array.astype(np.uint8)\n",
    "                img = Image.fromarray(img_array)\n",
    "                # Optionally resize image for better display\n",
    "                buffer = io.BytesIO()\n",
    "                img.save(buffer, format='PNG')\n",
    "                img_bytes = buffer.getvalue()\n",
    "                # Create Image widget\n",
    "                img_widget = widgets.Image(\n",
    "                    value=img_bytes,\n",
    "                    format='png',\n",
    "                    width=398,\n",
    "                    height=224\n",
    "                )\n",
    "                img_widgets.append(img_widget)\n",
    "            # Create GridBox layout\n",
    "            grid = widgets.GridBox(\n",
    "                img_widgets,\n",
    "                layout=widgets.Layout(\n",
    "                    grid_template_columns=f\"repeat({n_cols}, 224px)\",\n",
    "                    grid_gap='10px'\n",
    "                )\n",
    "            )\n",
    "            display(grid)\n",
    "\n",
    "    # Function to handle click events (toggle selection)\n",
    "    def on_click(trace, points, state):\n",
    "        if points.point_inds:\n",
    "            for idx in points.point_inds:\n",
    "                if idx in selected_indices:\n",
    "                    selected_indices.remove(idx)\n",
    "                else:\n",
    "                    selected_indices.add(idx)\n",
    "            # Update marker colors based on selection\n",
    "            with fig.batch_update():\n",
    "                trace.marker.color = [\n",
    "                    'red' if i in selected_indices else 'blue' for i in range(len(images))\n",
    "                ]\n",
    "            update_image_grid(selected_indices)\n",
    "\n",
    "    # Function to handle selection events (lasso or box select)\n",
    "    def on_select(trace, points, state):\n",
    "        if points.point_inds:\n",
    "            # Replace current selection with new selection\n",
    "            selected_indices.clear()\n",
    "            for idx in points.point_inds:\n",
    "                selected_indices.add(idx)\n",
    "            # Update marker colors based on selection\n",
    "            with fig.batch_update():\n",
    "                trace.marker.color = [\n",
    "                    'red' if i in selected_indices else 'blue' for i in range(len(images))\n",
    "                ]\n",
    "            update_image_grid(selected_indices)\n",
    "        else:\n",
    "            # If no points selected, clear selection\n",
    "            selected_indices.clear()\n",
    "            with fig.batch_update():\n",
    "                trace.marker.color = ['blue'] * len(images)\n",
    "            update_image_grid(selected_indices)\n",
    "\n",
    "    # Attach the click and select events to the scatter plot\n",
    "    fig.data[0].on_click(on_click)\n",
    "    fig.data[0].on_selection(on_select)\n",
    "\n",
    "    # Initial display message\n",
    "    with image_output:\n",
    "        display(widgets.HTML(\"<b>No images selected.</b>\"))\n",
    "\n",
    "    # Layout the plot and image grid side by side\n",
    "    hbox = widgets.HBox([fig, image_output])\n",
    "    display(hbox)\n"
   ],
   "id": "757d213ecd72b4cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_umap_with_images(reduced_embeddings, images)",
   "id": "b21b2dad2c34e94c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def similarity(query_index, data_dict, k=10):\n",
    "    \"\"\"    \n",
    "    Args:\n",
    "        query_index (int): Index of the query image in the data_dict.\n",
    "        data_dict (dict): \n",
    "            \"images\": List of images as NumPy arrays with shape (224, 398, 3).\n",
    "            \"embeddings\": List of embedding vectors, each of shape (768,).\n",
    "        k (int): Number of top similar images to return. Default is 10.\n",
    "    \"\"\"\n",
    "    embeddings_matrix = np.array(data_dict[\"embeddings\"])  # Shape: (n_samples, 768)\n",
    "\n",
    "    query_embedding = embeddings_matrix[query_index]        # Shape: (768,)\n",
    "    query_image = data_dict[\"images\"][query_index]           # Shape: (224, 398, 3)\n",
    "\n",
    "    embeddings_norm = embeddings_matrix / np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)\n",
    "    query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "    cosine_similarities = np.dot(embeddings_norm, query_norm)  # Shape: (n_samples,)\n",
    "\n",
    "    # Exclude the query itself\n",
    "    cosine_similarities[query_index] = -np.inf\n",
    "\n",
    "    # Get top k indices\n",
    "    top_k_indices = np.argsort(cosine_similarities)[-k:][::-1]  # Descending order\n",
    "\n",
    "    # Retrieve top k images and their similarity scores\n",
    "    top_k_images = [data_dict[\"images\"][i] for i in top_k_indices]\n",
    "    top_k_scores = [cosine_similarities[i] for i in top_k_indices]\n",
    "\n",
    "    plt.figure(figsize=(5 * (k + 1), 5)) \n",
    "\n",
    "    # Display the original (query) image\n",
    "    plt.subplot(1, k + 1, 1)\n",
    "    plt.imshow(query_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\", fontsize=20)\n",
    "\n",
    "    # Display the top k similar images\n",
    "    for idx, (img, score) in enumerate(zip(top_k_images, top_k_scores), start=2):\n",
    "        plt.subplot(1, k + 1, idx)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{score:.4f}\", fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Combine images and scores\n",
    "    top_k_results = list(zip(top_k_images, top_k_scores))\n",
    "\n",
    "    return top_k_results\n",
    "\n",
    "\n",
    "def similarity_range(data_dict):\n",
    "    query_embedding = np.ones(768, dtype=np.float32)\n",
    "    \n",
    "    embeddings_matrix = np.array(data_dict[\"embeddings\"])  # Shape: (n_samples, 768)\n",
    "\n",
    "    embeddings_norm = embeddings_matrix / np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)\n",
    "    query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "    cosine_similarities = np.dot(embeddings_norm, query_norm)  # Shape: (n_samples,)\n",
    "\n",
    "    return np.min(cosine_similarities), np.max(cosine_similarities)    "
   ],
   "id": "62cacde1307db8d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "similarity_range(im_vec_initial)",
   "id": "35a223350202da6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Similarity Range\n",
    "So this is the higlight of the problem. Since the foundational model is trained and designed to be used on diverse datasets, when inserted one datasets, items from it falls in tiny regoion of space. This is what is tended to be fixed, we want to increse the similarity scores range of the data, thus give more space for features to represent features that are relevant to this specific problem."
   ],
   "id": "96a6edec22d98cac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_k_results = similarity(1381, im_vec_initial, k=10)\n",
    "top_k_results = similarity(66, im_vec_initial, k=10)\n",
    "top_k_results = similarity(21, im_vec_initial, k=10)"
   ],
   "id": "a860f23680845da2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "The idea is to fine-tune the foundation model in unsupervised way with idea of encoder and decoder. Intuition is as follows - in this task the main idea is try to overfit the data as much as possible, since we dont use this embeddings in any multi-dataset tasks or meming retrieval, we only want embeddings to be good comparable to each other. This implies that if the decoder model could overfit on the main idea of dataset and image view, then embedding from encoder model will describe all the features that differ one image from another."
   ],
   "id": "d7b2031a42e983d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setup Train\n",
    "choose parameters to train on both Encoder and Decoder"
   ],
   "id": "31e83ee06fe98b3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "params_to_optimize = list(decoder.parameters())\n",
    "\n",
    "# Add the trainable parameters from the last 25% of ViT layers\n",
    "for i, layer in enumerate(model.encoder.layer):\n",
    "    if i >= freeze_layers:\n",
    "        params_to_optimize.extend(list(layer.parameters()))"
   ],
   "id": "17dd6c87346460e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(params_to_optimize, lr=1e-3)\n",
    "criterion = nn.MSELoss()"
   ],
   "id": "f469e667553f63a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from tqdm import tqdm  # Optional: For progress bars\n",
    "\n",
    "def train(num_epochs):\n",
    "    \"\"\"\n",
    "    Trains the model and decoder for a specified number of epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - num_epochs (int): The number of epochs to train the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Setup Device (CUDA if available)\n",
    "    # -------------------------------\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Move the model and decoder to the selected device\n",
    "    model.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Training Loop Over Epochs\n",
    "    # -------------------------------\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "        epoch_loss = 0.0  # To accumulate loss over the epoch\n",
    "\n",
    "        # Set models to training mode\n",
    "        model.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Optional: Use tqdm for a progress bar\n",
    "        progress_bar = tqdm(enumerate(loader), total=len(loader), desc=f\"Training Epoch {epoch}\")\n",
    "\n",
    "        for batch_idx, batch in progress_bar:\n",
    "            # -------------------------------\n",
    "            # 3. Prepare the Batch\n",
    "            # -------------------------------\n",
    "            # Assume batch shape is [Batch, Height, Width, Channels]\n",
    "            # Permute to [Batch, Channels, Height, Width]\n",
    "            batch = batch.permute(0, 3, 1, 2)  # New shape: [Batch, 3, 224, 224]\n",
    "\n",
    "            # Convert to float and normalize to [0, 1]\n",
    "            batch = batch.float() / 255.0\n",
    "\n",
    "            # Define target as the input batch (for reconstruction)\n",
    "            target = batch.clone()\n",
    "\n",
    "            # Move batch and target to the device\n",
    "            batch = batch.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # -------------------------------\n",
    "            # 4. Forward Pass\n",
    "            # -------------------------------\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            outputs = model(batch, output_hidden_states=False)  # Forward pass through the model\n",
    "            reconstruction = decoder(outputs.last_hidden_state)  # Decode the model's output\n",
    "\n",
    "            # -------------------------------\n",
    "            # 5. Compute Loss\n",
    "            # -------------------------------\n",
    "            loss = criterion(reconstruction, target)\n",
    "            epoch_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "            # -------------------------------\n",
    "            # 6. Backward Pass and Optimization\n",
    "            # -------------------------------\n",
    "            loss.backward()  # Backward pass to compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            # -------------------------------\n",
    "            # 7. Logging and Visualization\n",
    "            # -------------------------------\n",
    "            # Update the progress bar with the current loss\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "        # -------------------------------\n",
    "        # 8. Epoch Summary\n",
    "        # -------------------------------\n",
    "        avg_epoch_loss = epoch_loss / len(loader)\n",
    "        print(f\"Epoch {epoch} completed. Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        original_image = batch[0].detach().cpu()\n",
    "        reconstructed_image = reconstruction[0].detach().cpu()\n",
    "        show_image(original_image, reconstructed_image)\n",
    "\n"
   ],
   "id": "7490ba27563cea67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train(20)",
   "id": "ebed37ca48924bfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "reconstructed_images = decoder(outputs.last_hidden_state)",
   "id": "7f6ec7318726f09a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5705fad82411ec10",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
